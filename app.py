# -*- coding: utf-8 -*-
"""
StackMix OCR Web Interface
–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö OCR
"""

import gradio as gr
import subprocess
import os
import threading
import time
from pathlib import Path
import pandas as pd
import json
import torch
import cv2
import numpy as np
from configs.base import BaseConfig

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Unicode –ø—É—Ç—è–º–∏
def imwrite_unicode(path, img):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode –ø—É—Ç–µ–π"""
    ext = os.path.splitext(str(path))[1]
    result, encoded_img = cv2.imencode(ext, img)
    if result:
        with open(path, 'wb') as f:
            f.write(encoded_img)
        return True
    return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
current_process = None
current_thread = None
stop_event = None
progress_status = {'epoch': (0, 0), 'iter': (0, 0)}
tb_process = None
tb_port = 6006


def validate_dataset_path(data_dir, marking_csv_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Ñ–∞–π–ª–∞ —Ä–∞–∑–º–µ—Ç–∫–∏"""
    data_path = Path(data_dir)
    marking_file = Path(marking_csv_path)
    
    if not data_path.exists():
        return False, f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_path}"
    
    if not marking_file.exists():
        return False, f"–§–∞–π–ª —Ä–∞–∑–º–µ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {marking_file}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É CSV
    try:
        df = pd.read_csv(marking_file)
        required_columns = ['sample_id', 'path', 'stage', 'text']
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            return False, f"–í marking.csv –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–æ–ª–±—Ü—ã: {missing_cols}"
        
        return True, f"–î–∞—Ç–∞—Å–µ—Ç –Ω–∞–π–¥–µ–Ω ({len(df)} –∑–∞–ø–∏—Å–µ–π)"
    except Exception as e:
        return False, f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è marking.csv: {str(e)}"


def run_training_process(
    data_dir,
    marking_csv_path,
    experiment_name,
    output_dir,
    experiment_description,
    image_w,
    image_h,
    num_epochs,
    batch_size,
    num_workers,
    use_blot,
    use_augs,
    use_stackmix,
    use_pretrained_backbone,
    seed,
    checkpoint_path,
    neptune_project,
    neptune_token,
    mwe_tokens_dir,
    stop_evt=None
):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ TensorBoard"""
    global current_process, tb_process, tb_port
    
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        experiment_dir = os.path.join(output_dir, experiment_name)
        os.makedirs(experiment_dir, exist_ok=True)
        
        # –°–æ–∑–¥–∞—ë–º TensorBoard writer –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û
        try:
            from torch.utils.tensorboard import SummaryWriter
            tb_logdir = os.path.join(experiment_dir, 'tensorboard')
            os.makedirs(tb_logdir, exist_ok=True)
            writer = SummaryWriter(log_dir=tb_logdir)
            writer.add_text('config/experiment_name', experiment_name, 0)
            writer.add_text('config/description', experiment_description, 0)
            writer.add_text('config/dataset', marking_csv_path, 0)
            writer.add_text('config/params', f"epochs={num_epochs}, batch_size={batch_size}, image_size={image_w}x{image_h}", 0)
            writer.add_text('config/augmentations', f"blot={use_blot}, augs={use_augs}, stackmix={use_stackmix}", 0)
            writer.flush()
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ–º TensorBoard –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ–º –≤ –±—Ä–∞—É–∑–µ—Ä–µ
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ü–û–°–õ–ï —Å–æ–∑–¥–∞–Ω–∏—è writer —á—Ç–æ–±—ã —Ñ–∞–π–ª—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–∏
            if tb_process is None:
                try:
                    import sys
                    import webbrowser
                    # TensorBoard —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –ø–∞–ø–∫—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–Ω–µ –Ω–∞ tensorboard –≤–Ω—É—Ç—Ä–∏)
                    cmd = [sys.executable, '-m', 'tensorboard', '--logdir', experiment_dir, '--port', str(tb_port), '--bind_all']
                    tb_process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    time.sleep(3)  # –î–∞—ë–º –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ TensorBoard –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è
                    webbrowser.open(f'http://localhost:{tb_port}/')
                    writer.add_text('tensorboard/url', f'TensorBoard –æ—Ç–∫—Ä—ã—Ç: http://localhost:{tb_port}/', 0)
                    writer.flush()
                except Exception as e:
                    writer.add_text('tensorboard/error', f'–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å TensorBoard: {e}', 0)
                    writer.flush()
        except ImportError as e:
            raise ImportError("TensorBoard is required. Install it with: pip install tensorboard") from e
        
        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ Python
        import sys
        sys.path.insert(0, '.')
        
        from src.experiment import OCRExperiment
        from src.model import get_ocr_model
        from src.dataset import DatasetRetriever
        from src.ctc_labeling import CTCLabeling
        from src import utils
        from src.predictor import Predictor
        from src.metrics import string_accuracy, cer, wer
        from configs.base import BaseConfig
        import torch
        import albumentations as A
        from torch.utils.data import SequentialSampler, RandomSampler
        
        # –ß–∏—Ç–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        df = pd.read_csv(marking_csv_path, index_col='sample_id')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç–∏: –∑–∞–º–µ–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–ª–µ—à–∏ –Ω–∞ –ø—Ä—è–º—ã–µ
        df['path'] = df['path'].str.replace('\\', '/')
        
        # –í–ê–ñ–ù–û: –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN/–ø—É—Å—Ç—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
        df = df.dropna(subset=['text'])
        df = df[df['text'].astype(str).str.strip() != '']
        
        if len(df) == 0:
            raise ValueError("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")
        
        writer.add_text('dataset/info', f'–°—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)}', 0)
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤
        all_text = ' '.join(df['text'].astype(str).values)
        chars = ''.join(sorted(set(all_text)))
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
        config_name = f"custom_{experiment_name}"
        config = BaseConfig(
            data_dir=data_dir,
            dataset_name=config_name,
            image_w=image_w,
            image_h=image_h,
            chars=chars,
            corpus_name="custom"
        )
        
        config.params['experiment_name'] = experiment_name
        config.params['experiment_description'] = experiment_description
        config.params['bs'] = batch_size
        config.params['num_workers'] = num_workers
        config.params['num_epochs'] = num_epochs
        
        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')
        writer.add_text('config/device', str(device), 0)
        
        ctc_labeling = CTCLabeling(config)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
        transforms = None
        if use_blot and use_augs:
            from src.blot import get_blot_transforms
            transforms = A.Compose([
                get_blot_transforms(config),
                A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25),
                A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),
                A.ImageCompression(quality_range=(75, 100), p=0.5),
            ], p=1.0)
        elif use_blot:
            from src.blot import get_blot_transforms
            transforms = get_blot_transforms(config)
        elif use_augs:
            transforms = A.Compose([
                A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25),
                A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),
                A.ImageCompression(quality_range=(75, 100), p=0.5),
            ], p=1.0)
        
        # –î–∞—Ç–∞—Å–µ—Ç—ã
        train_dataset_kwargs = {'transforms': transforms}
        if use_stackmix and mwe_tokens_dir:
            from src.stackmix import StackMix
            stackmix = StackMix(
                mwe_tokens_dir=mwe_tokens_dir,
                data_dir=data_dir,
                dataset_name=config_name,
                image_h=image_h,
            )
            stackmix.load()
            train_dataset_kwargs['stackmix'] = stackmix
        
        df_train = df[~df['stage'].isin(['valid', 'test'])]
        train_dataset = DatasetRetriever(df_train, config, ctc_labeling, **train_dataset_kwargs)
        valid_dataset = DatasetRetriever(df[df['stage'] == 'valid'], config, ctc_labeling)
        test_dataset = DatasetRetriever(df[df['stage'] == 'test'], config, ctc_labeling)
        
        # –ú–æ–¥–µ–ª—å
        model = get_ocr_model(config, pretrained=bool(use_pretrained_backbone))
        model = model.to(device)
        
        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), **config.params['optimizer']['params'])
        
        # DataLoaders
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=RandomSampler(train_dataset),
            pin_memory=False,
            drop_last=True,
            num_workers=num_workers,
            collate_fn=utils.kw_collate_fn
        )
        valid_loader = torch.utils.data.DataLoader(
            valid_dataset,
            batch_size=batch_size,
            sampler=SequentialSampler(valid_dataset),
            pin_memory=False,
            drop_last=False,
            num_workers=num_workers,
            collate_fn=utils.kw_collate_fn
        )
        
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            epochs=num_epochs,
            steps_per_epoch=len(train_loader),
            **config.params['scheduler']['params'],
        )
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–ª–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ TensorBoard
        best_cer = float('inf')
        epoch_count = int(num_epochs)
        global_step = 0
        
        writer.add_text('training/start', f'–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {num_epochs} —ç–ø–æ—Ö', 0)
        writer.flush()
        
        for epoch in range(1, epoch_count + 1):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if stop_evt is not None and stop_evt.is_set():
                writer.add_text('training/status', f'–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ —ç–ø–æ—Ö–µ {epoch}', epoch)
                break

            model.train()
            epoch_losses = []
            n_batches = len(train_loader)
            for batch_idx, batch in enumerate(train_loader, start=1):
                if stop_evt is not None and stop_evt.is_set():
                    writer.add_text('training/status', f'–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—ç–ø–æ—Ö–∞ {epoch}, –±–∞—Ç—á {batch_idx})', global_step)
                    break

                # forward/backward
                lengths = batch['encoded_length'].to(device, dtype=torch.int32)
                encoded = batch['encoded'].to(device, dtype=torch.int32)
                outputs = model(batch['image'].to(device, dtype=torch.float32))

                preds_size = torch.IntTensor([outputs.size(1)] * batch['encoded'].shape[0])
                preds = outputs.log_softmax(2).permute(1, 0, 2)

                loss = criterion(preds, encoded, preds_size, lengths)
                loss_value = loss.detach().cpu().item()
                epoch_losses.append(loss_value)

                if torch.isfinite(loss):
                    loss.backward()
                    optimizer.step()
                    optimizer.zero_grad()
                    try:
                        scheduler.step()
                    except Exception:
                        pass

                # –õ–æ–≥–∏—Ä—É–µ–º –≤ TensorBoard –≤—Å—ë
                writer.add_scalar('train/loss', loss_value, global_step)
                writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], global_step)
                global_step += 1
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π flush (–∫–∞–∂–¥—ã–µ 100 –±–∞—Ç—á–µ–π)
                if batch_idx % 100 == 0:
                    writer.flush()

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress_status['epoch'] = (epoch, epoch_count)
                progress_status['iter'] = (batch_idx, n_batches)

            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ä–µ–¥–Ω—é—é loss –∑–∞ —ç–ø–æ—Ö—É
            avg_epoch_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0
            writer.add_scalar('train/avg_epoch_loss', avg_epoch_loss, epoch)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏
            model.eval()
            predictor = Predictor(model, device)
            try:
                predictions = predictor.run_inference(valid_loader)
            except Exception as e:
                writer.add_text('validation/error', f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}', epoch)
                predictions = []

            if predictions:
                df_pred = pd.DataFrame([{
                    'id': p['id'],
                    'pred_text': ctc_labeling.decode(p['raw_output'].argmax(1)),
                    'gt_text': p['gt_text']
                } for p in predictions]).set_index('id')

                cer_metric = round(cer(df_pred['pred_text'], df_pred['gt_text']), 5)
                wer_metric = round(wer(df_pred['pred_text'], df_pred['gt_text']), 5)
                acc_metric = round(string_accuracy(df_pred['pred_text'], df_pred['gt_text']), 5)

                writer.add_scalar('val/cer', cer_metric, epoch)
                writer.add_scalar('val/wer', wer_metric, epoch)
                writer.add_scalar('val/acc', acc_metric, epoch)
                writer.add_text('val/metrics', f'CER: {cer_metric}, WER: {wer_metric}, ACC: {acc_metric}', epoch)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π –ø–æ CER
                if cer_metric < best_cer:
                    best_cer = cer_metric
                    torch.save({'model_state_dict': model.state_dict(), 'cer': cer_metric}, os.path.join(experiment_dir, 'best_cer.pt'))
                    writer.add_text('checkpoints/best', f'–ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞ —ç–ø–æ—Ö–µ {epoch} —Å CER={cer_metric}', epoch)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                df_pred.to_csv(os.path.join(experiment_dir, f'pred__epoch_{epoch}.csv'))

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
            torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, os.path.join(experiment_dir, 'last.pt'))

        writer.add_text('training/status', '–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ', epoch_count)
        writer.flush()
        writer.close()
            
    except Exception as e:
        if 'writer' in locals():
            import traceback
            error_msg = traceback.format_exc()
            writer.add_text('training/error', f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}\n\n{error_msg}', 0)
            writer.flush()
            writer.close()
        raise
    finally:
        current_process = None


def auto_update_logs_loop():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ª–æ–≥–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    global auto_update_active
    while auto_update_active:
        time.sleep(1)
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–æ–π–¥—ë—Ç —á–µ—Ä–µ–∑ –∫–Ω–æ–ø–∫—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        

def handle_start_training(
    data_dir,
    marking_csv_path,
    experiment_name,
    output_dir,
    experiment_description,
    image_w,
    image_h,
    num_epochs,
    batch_size,
    num_workers,
    use_blot,
    use_augs,
    use_stackmix,
    use_pretrained_backbone,
    seed,
    checkpoint_path,
    neptune_project,
    neptune_token,
    mwe_tokens_dir
):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–∫–∏ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
    global current_thread, current_process, stop_event, auto_update_active
    
    if current_process is not None or (current_thread and current_thread.is_alive()):
        return "–û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ! –û—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –Ω–æ–≤–æ–≥–æ."
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    if not experiment_name.strip():
        return "–£–∫–∞–∂–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
    
    if not marking_csv_path.strip():
        return "–£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ marking.csv"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    is_valid, message = validate_dataset_path(data_dir, marking_csv_path)
    if not is_valid:
        return f"–û—à–∏–±–∫–∞: {message}"
    
    # –°–±—Ä–æ—Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    progress_status['epoch'] = (0, 0)
    progress_status['iter'] = (0, 0)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    stop_event = threading.Event()
    
    # –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –æ—Ç–ª–æ–≤–∞ –æ—à–∏–±–æ–∫
    def training_wrapper():
        try:
            run_training_process(
                data_dir, marking_csv_path, experiment_name, output_dir,
                experiment_description, image_w, image_h, num_epochs, batch_size,
                num_workers, use_blot, use_augs, use_stackmix,
                use_pretrained_backbone, seed, checkpoint_path, neptune_project,
                neptune_token, mwe_tokens_dir, stop_event
            )
        except Exception as e:
            import traceback
            error_msg = f"–û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –æ–±—É—á–µ–Ω–∏—è:\n{str(e)}\n\n{traceback.format_exc()}"
            print(error_msg)
            # –ü–∏—à–µ–º –≤ —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            try:
                os.makedirs(os.path.join(output_dir, experiment_name), exist_ok=True)
                with open(os.path.join(output_dir, experiment_name, 'error.log'), 'w', encoding='utf-8') as f:
                    f.write(error_msg)
            except Exception as e2:
                print(f"Failed to write error log: {e2}")
    
    current_thread = threading.Thread(target=training_wrapper)
    current_thread.daemon = True
    current_thread.start()
    current_process = True
    
    return f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ! –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {experiment_name}\n\nTensorBoard –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫—Ä–æ–µ—Ç—Å—è –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–∞ http://127.0.0.1:6006/"


def stop_training():
    """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
    global current_process, stop_event, current_thread

    if current_process is None:
        return "–û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω–æ"

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    if stop_event is not None:
        stop_event.set()

    # –î–æ–∂–¥—ë–º—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞ (–∫–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç)
    try:
        if current_thread is not None:
            current_thread.join(timeout=5)
    except Exception:
        pass

    current_process = None
    return "–û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ"


def start_tensorboard(output_dir, experiment_name, port=6006):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç tensorboard –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –Ω–∞ —Ñ–æ–Ω–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    global tb_process, tb_port
    tb_port = int(port)
    logdir = os.path.join(output_dir, experiment_name, 'tensorboard')
    if not os.path.exists(logdir):
        return False, f"Logdir –Ω–µ –Ω–∞–π–¥–µ–Ω: {logdir}"

    if tb_process is not None:
        return True, f"TensorBoard —É–∂–µ –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {tb_port}"

    try:
        import sys
        cmd = [sys.executable, '-m', 'tensorboard', '--logdir', logdir, '--port', str(tb_port), '--host', '127.0.0.1']
        tb_process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        time.sleep(1)
        return True, f"TensorBoard –∑–∞–ø—É—â–µ–Ω: http://127.0.0.1:{tb_port}/"
    except Exception as e:
        return False, f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å TensorBoard: {e}"


def stop_tensorboard():
    global tb_process
    if tb_process is None:
        return False, "TensorBoard –Ω–µ –∑–∞–ø—É—â–µ–Ω"
    try:
        tb_process.terminate()
        tb_process.wait(timeout=3)
    except Exception:
        try:
            tb_process.kill()
        except Exception:
            pass
    tb_process = None
    return True, "TensorBoard –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"


def start_tb_ui(output_dir, experiment_name, port):
    ok, msg = start_tensorboard(output_dir, experiment_name, port)
    return msg


def stop_tb_ui():
    ok, msg = stop_tensorboard()
    return msg


def check_dataset(data_dir, marking_csv_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç"""
    is_valid, message = validate_dataset_path(data_dir, marking_csv_path)
    
    if is_valid:
        df = pd.read_csv(marking_csv_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—ã–µ/NaN —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
        nan_count = df['text'].isna().sum()
        empty_count = (df['text'].astype(str).str.strip() == '').sum()
        
        train_count = len(df[df['stage'] == 'train'])
        valid_count = len(df[df['stage'] == 'valid'])
        test_count = len(df[df['stage'] == 'test'])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        missing_images = []
        for idx, row in df.head(5).iterrows():
            img_path = Path(data_dir) / row['path'].replace('\\', '/')
            if not img_path.exists():
                missing_images.append(f"  - {row['path']} -> {img_path}")
        
        missing_warning = ""
        if missing_images:
            missing_warning = f"""
**–í–ù–ò–ú–ê–ù–ò–ï: –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!**

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ `data_dir` —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–¥–∞–Ω—ã –ø—É—Ç–∏ –≤ CSV.

–ù–µ –Ω–∞–π–¥–µ–Ω—ã:
```
{chr(10).join(missing_images[:3])}
```

–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:
1. `data_dir` = –∫–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ (–≥–¥–µ –ª–µ–∂–∞—Ç train/, val/, –∏ —Ç.–¥.)
2. –ü—É—Ç–∏ –≤ CSV –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã –∫ `data_dir`
3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä—è–º—ã–µ —Å–ª–µ—à–∏ (/) –∏–ª–∏ –¥–≤–æ–π–Ω—ã–µ –æ–±—Ä–∞—Ç–Ω—ã–µ (\\\\) –≤ –ø—É—Ç—è—Ö
"""
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
        empty_warning = ""
        if nan_count > 0 or empty_count > 0:
            empty_warning = f"""
**‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã!**

- –ü—É—Å—Ç—ã—Ö (NaN): {nan_count}
- –ü—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {empty_count}

–≠—Ç–∏ —Å—Ç—Ä–æ–∫–∏ –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª–µ–Ω—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
"""
        
        info = f"""
**–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ**

- –°—Ç–∞—Ç—É—Å: –î–∞—Ç–∞—Å–µ—Ç –Ω–∞–π–¥–µ–Ω –∏ –≤–∞–ª–∏–¥–µ–Ω
- –ü—É—Ç—å –∫ marking.csv: {marking_csv_path}
- Data directory: {data_dir}
- –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}
- Train: {train_count}
- Valid: {valid_count}
- Test: {test_count}

{empty_warning}

{missing_warning}

**–ü–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏:**
```
{df.head(3).to_string()}
```
"""
        return info
    else:
        return f"–û—à–∏–±–∫–∞: {message}"


def prepare_char_masks(checkpoint_path, data_dir, marking_csv_path, image_w, image_h, batch_size, num_workers):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–∞—Å–∫–∏ —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        from src.dataset import DatasetRetriever
        from src.ctc_labeling import CTCLabeling
        from src.model import get_ocr_model
        from src.predictor import Predictor
        from src.char_masks import CharMasks
        from src import utils
        from torch.utils.data import SequentialSampler
        import json
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º checkpoint
        if not Path(checkpoint_path).exists():
            return f"–û—à–∏–±–∫–∞: Checkpoint –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}"
        
        if not Path(marking_csv_path).exists():
            return f"–û—à–∏–±–∫–∞: marking.csv –Ω–µ –Ω–∞–π–¥–µ–Ω: {marking_csv_path}"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –° –ò–ù–î–ï–ö–°–û–ú sample_id - —ç—Ç–æ –≤–∞–∂–Ω–æ!
        df = pd.read_csv(marking_csv_path, index_col='sample_id')
        df['path'] = df['path'].str.replace('\\', '/')
        
        # –í–ê–ñ–ù–û: –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN/–ø—É—Å—Ç—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
        df = df.dropna(subset=['text'])
        df = df[df['text'].astype(str).str.strip() != '']
        
        if len(df) == 0:
            return "–û—à–∏–±–∫–∞: –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!"
        
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        all_chars = set()
        for text in df['text'].values:
            all_chars.update(str(text))
        chars = ''.join(sorted(list(all_chars)))
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
        config = BaseConfig(
            data_dir=data_dir,
            dataset_name='custom',
            chars=chars,
            corpus_name='custom_corpus',
            image_w=int(image_w),
            image_h=int(image_h),
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        config.params['experiment_name'] = 'char_masks_prep'
        config.params['experiment_description'] = 'Preparing char masks'
        config.params['num_epochs'] = 0
        config.params['bs'] = int(batch_size)
        config.params['num_workers'] = int(num_workers)
        
        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')
        
        # CTCLabeling
        ctc_labeling = CTCLabeling(config)
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        train_df = df[~df['stage'].isin(['valid', 'test'])]
        train_dataset = DatasetRetriever(train_df, config, ctc_labeling)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model = get_ocr_model(config)
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        model = model.to(device)
        
        # DataLoader
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=config['bs'],
            sampler=SequentialSampler(train_dataset),
            pin_memory=False,
            drop_last=False,
            num_workers=config['num_workers'],
            collate_fn=utils.kw_collate_fn
        )
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        predictor = Predictor(model, device)
        train_inference = predictor.run_inference(train_loader)
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫–∏
        char_masks = CharMasks(config, ctc_labeling, add=0, blank_add=0)
        all_masks, bad = char_masks.run(train_inference)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy int64 –≤ –æ–±—ã—á–Ω—ã–µ int –¥–ª—è JSON
        def convert_to_serializable(obj):
            """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç numpy —Ç–∏–ø—ã –≤ Python —Ç–∏–ø—ã"""
            if isinstance(obj, dict):
                return {key: convert_to_serializable(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            elif hasattr(obj, 'item'):  # numpy types
                return obj.item()
            return obj
        
        all_masks_serializable = convert_to_serializable(all_masks)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–æ–¥–ø–∞–ø–∫—É custom (–∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç StackMix)
        output_subdir = Path(data_dir) / 'custom'
        output_subdir.mkdir(parents=True, exist_ok=True)
        output_path = output_subdir / 'all_char_masks.json'
        
        with open(output_path, 'w') as f:
            json.dump(all_masks_serializable, f)
        
        return f"‚úì –ú–∞—Å–∫–∏ —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!\n\n–•–æ—Ä–æ—à–∏–µ –º–∞—Å–∫–∏: {len(all_masks)}\n–ü–ª–æ—Ö–∏–µ –º–∞—Å–∫–∏: {len(bad)}\n\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}"
        
    except Exception as e:
        import traceback
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –º–∞—Å–æ–∫:\n{str(e)}\n\n{traceback.format_exc()}"


def prepare_stackmix_tokens(data_dir, marking_csv_path, image_h, tokens_dir, num_workers):
    """–®–ê–ì 2: –°–æ–∑–¥–∞—ë—Ç —Ç–æ–∫–µ–Ω—ã StackMix –∏–∑ –º–∞—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤"""
    try:
        from src.stackmix import StackMix
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º all_char_masks.json –≤ –ø–æ–¥–ø–∞–ø–∫–µ custom
        masks_path = Path(data_dir) / 'custom' / 'all_char_masks.json'
        if not masks_path.exists():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–∫–∂–µ –≤ –∫–æ—Ä–Ω–µ (–Ω–∞ —Å–ª—É—á–∞–π —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞)
            old_masks_path = Path(data_dir) / 'all_char_masks.json'
            if old_masks_path.exists():
                # –ö–æ–ø–∏—Ä—É–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ
                import shutil
                masks_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy(old_masks_path, masks_path)
            else:
                return f"–û—à–∏–±–∫–∞: –°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –º–∞—Å–∫–∏ —Å–∏–º–≤–æ–ª–æ–≤ (–®–∞–≥ 1)!\n\n–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {masks_path}"
        
        if not Path(marking_csv_path).exists():
            return f"–û—à–∏–±–∫–∞: marking.csv –Ω–µ –Ω–∞–π–¥–µ–Ω: {marking_csv_path}"
        
        # –°–æ–∑–¥–∞–µ–º mwe_tokens_dir
        mwe_tokens_dir = Path(tokens_dir)
        mwe_tokens_dir.mkdir(parents=True, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É –° –ò–ù–î–ï–ö–°–û–ú sample_id
        marking = pd.read_csv(marking_csv_path, index_col='sample_id')
        marking['path'] = marking['path'].str.replace('\\', '/')
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN/–ø—É—Å—Ç—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
        marking = marking.dropna(subset=['text'])
        marking = marking[marking['text'].astype(str).str.strip() != '']
        
        if len(marking) == 0:
            return "–û—à–∏–±–∫–∞: –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!"
        
        # –°–æ–∑–¥–∞–µ–º StackMix
        stackmix = StackMix(
            mwe_tokens_dir=str(mwe_tokens_dir),
            data_dir=data_dir,
            dataset_name='custom',
            image_h=int(image_h),
            p_background_smoothing=0.1
        )
        
        status = "=== –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ StackMix ===\n\n"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
        train_records = marking[~marking['stage'].isin(['valid', 'test'])]
        train_count = len(train_records)
        status += f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {train_count}\n"
        
        if train_count == 0:
            return "–û—à–∏–±–∫–∞: –í –¥–∞—Ç–∞—Å–µ—Ç–µ –Ω–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (stage='train')!"
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø—É—Ç–µ–π
        sample_paths = train_records['path'].head(3).tolist()
        status += f"–ü—Ä–∏–º–µ—Ä—ã –ø—É—Ç–µ–π:\n"
        for p in sample_paths:
            full_path = Path(data_dir) / p
            status += f"  - {p} -> {'‚úì' if full_path.exists() else '‚úó'}\n"
        status += "\n"
        
        status += f"–ó–∞–ø—É—Å–∫ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (workers={num_workers})...\n"
        
        try:
            stackmix.prepare_stackmix_dir(marking, num_workers=int(num_workers))
        except Exception as e:
            import traceback
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤:\n{str(e)}\n\n{traceback.format_exc()}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            stackmix.load()
            if stackmix.stackmix_data is None or len(stackmix.stackmix_data) == 0:
                return "–û—à–∏–±–∫–∞: –¢–æ–∫–µ–Ω—ã –Ω–µ —Å–æ–∑–¥–∞–Ω—ã! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º."
            
            status += f"\n‚úì –¢–æ–∫–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã!\n\n"
            status += f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n"
            status += f"  - –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(stackmix.stackmix_data)}\n"
            status += f"  - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {stackmix.stackmix_data['text'].nunique()}\n"
            status += f"  - –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {mwe_tokens_dir}/custom/\n\n"
            status += f"üìÅ –ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∞–º: {mwe_tokens_dir}\n\n"
            status += "–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ –®–∞–≥—É 3 - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–æ–∫–µ–Ω–æ–≤:\n{str(e)}"
        
        return status
        
    except Exception as e:
        import traceback
        return f"–û—à–∏–±–∫–∞:\n{str(e)}\n\n{traceback.format_exc()}"


def generate_images_from_corpus(tokens_dir, data_dir, marking_csv_path, text_file, image_h, output_dir, num_samples):
    """–®–ê–ì 3: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
    try:
        from src.stackmix import StackMix
        from src.ctc_labeling import CTCLabeling
        import cv2
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∞–º, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑–∞–ª custom –Ω–∞ –∫–æ–Ω—Ü–µ
        mwe_tokens_dir = Path(tokens_dir)
        if mwe_tokens_dir.name.lower() == 'custom':
            mwe_tokens_dir = mwe_tokens_dir.parent
        real_tokens_dir = mwe_tokens_dir / 'custom'
        stackmix_csv = real_tokens_dir / 'stackmix.csv'
        
        if not stackmix_csv.exists():
            return f"–û—à–∏–±–∫–∞: –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ —Ç–æ–∫–µ–Ω—ã (–®–∞–≥ 2)!\n\n–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {stackmix_csv}"
        
        if not text_file or not Path(text_file).exists():
            return f"–û—à–∏–±–∫–∞: –£–∫–∞–∂–∏—Ç–µ —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏!"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤
        marking = pd.read_csv(marking_csv_path, index_col='sample_id')
        marking = marking.dropna(subset=['text'])
        
        all_chars = set()
        for text in marking['text'].values:
            all_chars.update(str(text))
        chars = ''.join(sorted(list(all_chars)))
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
        config = BaseConfig(
            data_dir=data_dir,
            dataset_name='custom',
            chars=chars,
            corpus_name='custom_corpus',
            image_w=512,
            image_h=int(image_h),
        )
        
        ctc_labeling = CTCLabeling(config)
        
        # –°–æ–∑–¥–∞–µ–º StackMix –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        stackmix = StackMix(
            mwe_tokens_dir=str(mwe_tokens_dir),
            data_dir=data_dir,
            dataset_name='custom',
            image_h=int(image_h),
            p_background_smoothing=0.1
        )
        stackmix.load()
        
        status = "=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===\n\n"
        status += f"–ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∞–º: {real_tokens_dir}\n"
        status += f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(stackmix.stackmix_data)}\n"
        status += f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {stackmix.stackmix_data['text'].nunique()}\n\n"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–º–≤–æ–ª—ã –≤ –∫–æ—Ä–ø—É—Å–µ
        corpus_chars = set()
        with open(text_file, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
            for line in lines:
                corpus_chars.update(line)
        
        dataset_chars = set(chars)
        missing_chars = corpus_chars - dataset_chars - {'\n', '\r', '\t', ' '}
        
        if missing_chars:
            status += f"‚ö†Ô∏è –í –∫–æ—Ä–ø—É—Å–µ –µ—Å—Ç—å —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:\n"
            status += f"   {sorted(missing_chars)[:20]}{'...' if len(missing_chars) > 20 else ''}\n\n"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ—Ä–ø—É—Å (–∫–æ–ø–∏—Ä—É–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Ä—è–¥–æ–º —Å —Ç–æ–∫–µ–Ω–∞–º–∏)
        corpus_temp = mwe_tokens_dir / 'corpus_temp.txt'
        import shutil
        shutil.copy(text_file, corpus_temp)
        
        stackmix.load_corpus(ctc_labeling, str(corpus_temp))
        status += f"–ö–æ—Ä–ø—É—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {len(stackmix.corpus)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç—Ä–æ–∫\n\n"
        
        if len(stackmix.corpus) == 0:
            return status + "–û—à–∏–±–∫–∞: –í –∫–æ—Ä–ø—É—Å–µ –Ω–µ—Ç —Å—Ç—Ä–æ–∫ —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏!"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º (—Å–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        gen_dir = Path(output_dir)
        gen_dir.mkdir(parents=True, exist_ok=True)
        
        actual_samples = min(int(num_samples), len(stackmix.corpus))
        status += f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {actual_samples} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)...\n"
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import random
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∑–∞—Ä–∞–Ω–µ–µ (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å race condition)
        selected_texts = [random.choice(stackmix.corpus) for _ in range(actual_samples)]
        
        generated = []
        errors = []
        errors_lock = threading.Lock()
        generated_lock = threading.Lock()
        
        def generate_single(idx, text):
            """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
            try:
                image = stackmix.run_stackmix(text)
                if image is not None:
                    img_path = gen_dir / f'gen_{idx:05d}.png'
                    imwrite_unicode(img_path, image)
                    return {
                        'sample_id': idx,
                        'path': f'gen_{idx:05d}.png',
                        'text': text,
                        'stage': 'train'
                    }
                else:
                    return None, f"–ü–æ–ø—ã—Ç–∫–∞ {idx}: run_stackmix –≤–µ—Ä–Ω—É–ª None"
            except Exception as e:
                return None, f"–ü–æ–ø—ã—Ç–∫–∞ {idx}: {type(e).__name__}: {str(e)}"
        
        num_workers = min(8, actual_samples)  # –ù–µ –±–æ–ª—å—à–µ 8 –≤–æ—Ä–∫–µ—Ä–æ–≤
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = {executor.submit(generate_single, i, text): i 
                      for i, text in enumerate(selected_texts)}
            
            for future in as_completed(futures):
                result = future.result()
                if isinstance(result, dict):
                    with generated_lock:
                        generated.append(result)
                elif isinstance(result, tuple) and result[0] is None:
                    with errors_lock:
                        errors.append(result[1])
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ sample_id –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        generated.sort(key=lambda x: x['sample_id'])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ç–∫—É
        gen_marking = pd.DataFrame(generated)
        marking_path = gen_dir / 'marking.csv'
        gen_marking.to_csv(marking_path, index=False)
        
        status += f"\n‚úì –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n\n"
        status += f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç:\n"
        status += f"  - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {len(generated)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n"
        status += f"  - –û—à–∏–±–æ–∫: {len(errors)}\n"
        
        if errors:
            status += f"\n‚ö†Ô∏è –û—à–∏–±–∫–∏:\n"
            for err in errors[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                status += f"  - {err}\n"
            if len(errors) > 5:
                status += f"  ... –∏ –µ—â—ë {len(errors) - 5} –æ—à–∏–±–æ–∫\n"
        
        status += f"\n  - –ü–∞–ø–∫–∞: {gen_dir}\n"
        status += f"  - –†–∞–∑–º–µ—Ç–∫–∞: {marking_path}\n\n"
        
        if len(generated) > 0:
            status += "‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!"
        else:
            status += "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—à–∏–±–∫–∏ –≤—ã—à–µ."
        
        return status
        
    except Exception as e:
        import traceback
        return f"–û—à–∏–±–∫–∞:\n{str(e)}\n\n{traceback.format_exc()}"






# –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
with gr.Blocks(title="StackMix OCR Training") as app:
    gr.Markdown("""
    # StackMix OCR - –û–±—É—á–µ–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    
    **–í–∞–∂–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—É—Ç—è–º–∏:**
    - `data_dir` –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–æ–π, –æ—Ç–∫—É–¥–∞ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –ø—É—Ç–∏ –≤ marking.csv
    - –ü—É—Ç–∏ –≤ CSV –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∫ `data_dir`
    - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä—è–º—ã–µ —Å–ª–µ—à–∏ `/` –∏–ª–∏ –¥–≤–æ–π–Ω—ã–µ –æ–±—Ä–∞—Ç–Ω—ã–µ `\\\\` –≤ –ø—É—Ç—è—Ö
    
    **–ü—Ä–∏–º–µ—Ä:**
    - data_dir: `C:/Users/USER/Desktop/dataset`
    - –ø—É—Ç—å –≤ CSV: `train/img/image1.png`
    - –ø–æ–ª–Ω—ã–π –ø—É—Ç—å: `C:/Users/USER/Desktop/dataset/train/img/image1.png`
    """)
    
    with gr.Tabs():
        # ========== –í–ö–õ–ê–î–ö–ê: –û–ë–£–ß–ï–ù–ò–ï ==========
        with gr.Tab("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
                    
                    data_dir = gr.Textbox(
                        label="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏",
                        value="./data",
                        placeholder="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏"
                    )
                    
                    marking_csv_path = gr.Textbox(
                        label="–ü—É—Ç—å –∫ marking.csv",
                        value="./data/marking.csv",
                        placeholder="–ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É marking.csv"
                    )
                    
                    experiment_name = gr.Textbox(
                        label="–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞",
                        value="OCR_experiment",
                        placeholder="my_experiment_v1"
                    )
                    
                    output_dir = gr.Textbox(
                        label="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                        value="./exp",
                        placeholder="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"
                    )
                    
                    experiment_description = gr.Textbox(
                        label="–û–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞",
                        value="Training OCR model",
                        lines=2
                    )
                    
                    check_dataset_btn = gr.Button("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç", size="sm")
                    dataset_info = gr.Markdown("")
                    
                with gr.Column(scale=1):
                    gr.Markdown("### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
                    
                    with gr.Row():
                        image_w = gr.Number(label="–®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", value=256, precision=0)
                        image_h = gr.Number(label="–í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", value=32, precision=0)
                    
                    with gr.Row():
                        num_epochs = gr.Number(label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö", value=200, precision=0)
                        batch_size = gr.Number(label="Batch size", value=256, precision=0)
                    
                    with gr.Row():
                        num_workers = gr.Number(label="Num workers", value=8, precision=0)
                        seed = gr.Number(label="Random seed", value=6955, precision=0)
                    
                    gr.Markdown("### –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
                    
                    with gr.Row():
                        use_augs = gr.Checkbox(label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", value=True)
                        use_blot = gr.Checkbox(label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Blot", value=False)
                        use_stackmix = gr.Checkbox(label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å StackMix", value=False)
                    
                    gr.Markdown("### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
                    
                    use_pretrained_backbone = gr.Checkbox(
                        label="Pretrained backbone", 
                        value=True
                    )
                    
                    checkpoint_path = gr.Textbox(
                        label="Checkpoint –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                        value="",
                        placeholder="–û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –¥–ª—è –Ω–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
                    )
            
            with gr.Accordion("Neptune AI (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", open=False):
                with gr.Row():
                    neptune_project = gr.Textbox(
                        label="Neptune Project",
                        placeholder="username/project-name",
                        value=""
                    )
                    neptune_token = gr.Textbox(
                        label="Neptune API Token",
                        type="password",
                        value=""
                    )
            
            with gr.Accordion("StackMix –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", open=False):
                mwe_tokens_dir = gr.Textbox(
                    label="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è MWE tokens",
                    value="",
                    placeholder="–ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∞–º –¥–ª—è StackMix"
                )
            
            gr.Markdown("---")
            
            with gr.Row():
                start_btn = gr.Button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ", variant="primary", size="lg")
                stop_btn = gr.Button("–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ", variant="stop", size="lg")

            with gr.Row():
                tb_port = gr.Number(label="TensorBoard port", value=6006, precision=0)
                start_tb_btn = gr.Button("–ó–∞–ø—É—Å—Ç–∏—Ç—å TensorBoard –≤—Ä—É—á–Ω—É—é", size="sm")
                stop_tb_btn = gr.Button("–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å TensorBoard", size="sm")
                tb_status = gr.Textbox(label="TensorBoard status", interactive=False, lines=1)
            
            status_msg = gr.Textbox(label="–°—Ç–∞—Ç—É—Å", interactive=False, lines=3)

        
        # ========== –í–ö–õ–ê–î–ö–ê: –ì–ï–ù–ï–†–ê–¶–ò–Ø ==========
        with gr.Tab("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"):
            gr.Markdown("""
            ### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é StackMix
            
            **–ü—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ç—Ä–∏ —à–∞–≥–∞:**
            
            1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –º–∞—Å–∫–∏ —Å–∏–º–≤–æ–ª–æ–≤** - –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞—Å–∫–∏ –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ‚Üí `all_char_masks.json`
            2. **–°–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω—ã** - –Ω–∞—Ä–µ–∑–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Ç–æ–∫–µ–Ω—ã (n-–≥—Ä–∞–º–º—ã —Å–∏–º–≤–æ–ª–æ–≤)
            3. **–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è** - –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –ø–æ —Ç–µ–∫—Å—Ç–∞–º –∏–∑ –∫–æ—Ä–ø—É—Å–∞
            """)
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö")
                    gen_checkpoint_path = gr.Textbox(
                        label="–ü—É—Ç—å –∫ checkpoint –º–æ–¥–µ–ª–∏",
                        placeholder="exp/my_experiment/best_cer.pt"
                    )
                    
                    gen_data_dir = gr.Textbox(
                        label="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏",
                        value="./data"
                    )
                    
                    gen_marking_csv = gr.Textbox(
                        label="–ü—É—Ç—å –∫ marking.csv",
                        value="./data/marking.csv"
                    )
                    
                    gen_text_file = gr.Textbox(
                        label="–§–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (txt)",
                        placeholder="path/to/text_corpus.txt",
                        info="–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª, –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"
                    )
                    
                with gr.Column():
                    gr.Markdown("#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
                    gen_image_w = gr.Number(label="–®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", value=512, precision=0)
                    gen_image_h = gr.Number(label="–í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", value=64, precision=0)
                    gen_batch_size = gr.Number(label="Batch size (–¥–ª—è –º–∞—Å–æ–∫)", value=128, precision=0)
                    gen_num_workers = gr.Number(label="Num workers", value=8, precision=0)
                    gen_num_samples = gr.Number(label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", value=1000, precision=0)
                    
                    gen_tokens_dir = gr.Textbox(
                        label="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ (–®–∞–≥ 2)",
                        value="./stackmix_tokens",
                        info="–ü—É—Ç—å –≥–¥–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Ç–æ–∫–µ–Ω—ã"
                    )
                    
                    gen_output_dir = gr.Textbox(
                        label="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–®–∞–≥ 3)",
                        value="./generated_data",
                        info="–ü—É—Ç—å –≥–¥–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
                    )
            
            gr.Markdown("---")
            
            with gr.Row():
                prepare_masks_btn = gr.Button("1Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –º–∞—Å–∫–∏", variant="primary", size="lg")
                prepare_tokens_btn = gr.Button("2Ô∏è‚É£ –°–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω—ã", variant="primary", size="lg")
                generate_images_btn = gr.Button("3Ô∏è‚É£ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", variant="secondary", size="lg")
            
            gen_status = gr.Textbox(label="–°—Ç–∞—Ç—É—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", interactive=False, lines=10)
    
    # –ü—Ä–∏–≤—è–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ - –û–ë–£–ß–ï–ù–ò–ï
    check_dataset_btn.click(
        fn=check_dataset,
        inputs=[data_dir, marking_csv_path],
        outputs=dataset_info
    )
    
    start_btn.click(
        fn=handle_start_training,
        inputs=[
            data_dir, marking_csv_path, experiment_name, output_dir,
            experiment_description, image_w, image_h, num_epochs, batch_size,
            num_workers, use_blot, use_augs, use_stackmix,
            use_pretrained_backbone, seed, checkpoint_path, neptune_project,
            neptune_token, mwe_tokens_dir
        ],
        outputs=status_msg
    )
    
    stop_btn.click(
        fn=stop_training,
        inputs=None,
        outputs=status_msg
    )

    # TensorBoard controls
    start_tb_btn.click(
        fn=start_tb_ui,
        inputs=[output_dir, experiment_name, tb_port],
        outputs=tb_status
    )

    stop_tb_btn.click(
        fn=stop_tb_ui,
        inputs=None,
        outputs=tb_status
    )
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    prepare_masks_btn.click(
        fn=prepare_char_masks,
        inputs=[
            gen_checkpoint_path, gen_data_dir, gen_marking_csv,
            gen_image_w, gen_image_h, gen_batch_size, gen_num_workers
        ],
        outputs=gen_status
    )
    
    prepare_tokens_btn.click(
        fn=prepare_stackmix_tokens,
        inputs=[
            gen_data_dir, gen_marking_csv,
            gen_image_h, gen_tokens_dir, gen_num_workers
        ],
        outputs=gen_status
    )
    
    generate_images_btn.click(
        fn=generate_images_from_corpus,
        inputs=[
            gen_tokens_dir, gen_data_dir, gen_marking_csv, gen_text_file,
            gen_image_h, gen_output_dir, gen_num_samples
        ],
        outputs=gen_status
    )


if __name__ == "__main__":
    print("–ó–∞–ø—É—Å–∫ StackMix OCR Web Interface...")
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        inbrowser=True
    )
